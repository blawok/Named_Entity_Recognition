{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sagemaker\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/ner_dataset.csv\", encoding=\"latin1\").fillna(method=\"ffill\")\n",
    "# data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35178"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(set(data[\"Word\"].values))\n",
    "n_words = len(words); n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = list(set(data[\"Tag\"].values))\n",
    "n_tags = len(tags); n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)\n",
    "sentences = getter.sentences\n",
    "\n",
    "labels = [[s[2] for s in sent] for sent in sentences]\n",
    "sentences = [\" \".join([s[0] for s in sent]) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cnt = Counter(data[\"Word\"].values)\n",
    "vocabulary = set(w[0] for w in word_cnt.most_common(5000))\n",
    "\n",
    "max_len = 50\n",
    "word2idx = {\"PAD\": 0, \"UNK\": 1}\n",
    "word2idx.update({w: i for i, w in enumerate(words) if w in vocabulary})\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "\n",
    "X = [[word2idx.get(w, word2idx[\"UNK\"]) for w in s.split()] for s in sentences]\n",
    "\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "y = [[tag2idx[l_i] for l_i in l] for l in labels]\n",
    "\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1, shuffle=True, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "prefix = 'sagemaker/named_entity_recognition'\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(y_tr), pd.DataFrame(X_tr)], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using GPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m, \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mkeras\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mkeras\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m backend \u001b[34mas\u001b[39;49;00m K\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mkeras.models\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Sequential\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mkeras.layers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LSTM, Embedding, Dense, TimeDistributed, SpatialDropout1D, Bidirectional\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        \n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m5\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\n",
      "    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max-len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m50\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n-tags\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m17\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n-words\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m35178\u001b[39;49;00m)   \n",
      "    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--training\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    \n",
      "    args, _ = parser.parse_known_args()\n",
      "    \n",
      "    max_len    = args.max_len\n",
      "    n_tags     = args.n_tags\n",
      "    n_words    = args.n_words\n",
      "    epochs     = args.epochs\n",
      "    batch_size = args.batch_size\n",
      "    model_dir  = args.model_dir\n",
      "    training_dir   = args.training\n",
      "    \n",
      "    \u001b[37m# ----- LOAD DATA -----\u001b[39;49;00m\n",
      "    train_sample = pd.read_csv(os.path.join(training_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\n",
      "                               header=\u001b[36mNone\u001b[39;49;00m,\n",
      "                               names=\u001b[36mNone\u001b[39;49;00m,\n",
      "                               nrows=\u001b[34m1024\u001b[39;49;00m\n",
      "                              )\n",
      "\n",
      "    train_sample_y = train_sample.iloc[:,:-\u001b[34m50\u001b[39;49;00m].values\n",
      "    train_sample_X = train_sample.iloc[:,\u001b[34m50\u001b[39;49;00m:].values\n",
      "    \n",
      "    \u001b[37m# ----- DECLARE MODEL -----\u001b[39;49;00m\n",
      "    model = Sequential([\n",
      "\n",
      "        Embedding(input_dim=n_words, output_dim=\u001b[34m50\u001b[39;49;00m, input_length=max_len),\n",
      "        SpatialDropout1D(\u001b[34m0.1\u001b[39;49;00m),\n",
      "\n",
      "        Bidirectional(LSTM(units=\u001b[34m100\u001b[39;49;00m, return_sequences=\u001b[36mTrue\u001b[39;49;00m, recurrent_dropout=\u001b[34m0.1\u001b[39;49;00m)),\n",
      "        TimeDistributed(Dense(n_tags, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    ])\n",
      "    \n",
      "    model.compile(optimizer=\u001b[33m\"\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                  loss=\u001b[33m\"\u001b[39;49;00m\u001b[33msparse_categorical_crossentropy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    model.fit(train_sample_X, \n",
      "              train_sample_y.reshape(*train_sample_y.shape, \u001b[34m1\u001b[39;49;00m),\n",
      "              batch_size=batch_size,\n",
      "              epochs=epochs,\n",
      "              validation_split=\u001b[34m0.1\u001b[39;49;00m,\n",
      "              verbose=\u001b[34m1\u001b[39;49;00m)\n",
      "    \n",
      "\u001b[37m#     if args.current_host == args.hosts[0]:\u001b[39;49;00m\n",
      "\u001b[37m#         model.save(os.path.join(model_dir, '000000001'), 'bilstm.h5')\u001b[39;49;00m\n",
      "        \n",
      "\u001b[37m#     tf.saved_model.save(model,\u001b[39;49;00m\n",
      "\u001b[37m#                         os.path.join(model_dir, '1/'))\u001b[39;49;00m\n",
      "    \n",
      "    tf.saved_model.simple_save(\n",
      "            tf.keras.backend.get_session(),\n",
      "            os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\n",
      "            inputs={\u001b[33m'\u001b[39;49;00m\u001b[33minputs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: model.input},\n",
      "            outputs={t.name: t \u001b[34mfor\u001b[39;49;00m t \u001b[35min\u001b[39;49;00m model.outputs})\n"
     ]
    }
   ],
   "source": [
    "! pygmentize train/train_bilstm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator = TensorFlow(entry_point='train_bilstm.py', \n",
    "                          source_dir=\"train\",\n",
    "                          model_dir = '/opt/ml/model',\n",
    "                          role=role,\n",
    "                          train_instance_count=1, \n",
    "                          train_instance_type='ml.p2.xlarge',\n",
    "                          framework_version='1.14.0', \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={\n",
    "                              'epochs': 3,\n",
    "                              'batch-size': 32,\n",
    "                              'max-len': max_len,\n",
    "                              'n-tags': n_tags,\n",
    "                              'n-words': n_words\n",
    "                          }\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-22 09:11:25 Starting - Starting the training job...\n",
      "2020-05-22 09:11:27 Starting - Launching requested ML instances.........\n",
      "2020-05-22 09:13:00 Starting - Preparing the instances for training......\n",
      "2020-05-22 09:14:04 Downloading - Downloading input data...\n",
      "2020-05-22 09:14:48 Training - Downloading the training image.....\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[34mWARNING: Logging before flag parsing goes to stderr.\u001b[0m\n",
      "\u001b[34mW0522 09:15:35.988664 140591339124480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0522 09:15:36.010358 140591339124480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0522 09:15:36.013249 140591339124480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0522 09:15:36.033007 140591339124480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0522 09:15:36.041248 140591339124480 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\u001b[0m\n",
      "\u001b[34mW0522 09:15:36.785943 140591339124480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0522 09:15:36.806914 140591339124480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0522 09:15:36.948318 140591339124480 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.where in 2.0, which has the same broadcast rule as np.where\u001b[0m\n",
      "\u001b[34mTrain on 921 samples, validate on 103 samples\u001b[0m\n",
      "\u001b[34mEpoch 1/3\u001b[0m\n",
      "\n",
      "2020-05-22 09:15:29 Training - Training image download completed. Training in progress.\u001b[34m 32/921 [>.............................] - ETA: 2:07 - loss: 2.8356\n",
      " 64/921 [=>............................] - ETA: 1:03 - loss: 2.8181\n",
      " 96/921 [==>...........................] - ETA: 42s - loss: 2.7997 \u001b[0m\n",
      "\u001b[34m128/921 [===>..........................] - ETA: 31s - loss: 2.7801\u001b[0m\n",
      "\u001b[34m160/921 [====>.........................] - ETA: 25s - loss: 2.7588\u001b[0m\n",
      "\u001b[34m192/921 [=====>........................] - ETA: 20s - loss: 2.7352\u001b[0m\n",
      "\u001b[34m224/921 [======>.......................] - ETA: 17s - loss: 2.7096\u001b[0m\n",
      "\u001b[34m256/921 [=======>......................] - ETA: 15s - loss: 2.6807\u001b[0m\n",
      "\u001b[34m288/921 [========>.....................] - ETA: 13s - loss: 2.6431\u001b[0m\n",
      "\u001b[34m320/921 [=========>....................] - ETA: 11s - loss: 2.6010\u001b[0m\n",
      "\u001b[34m352/921 [==========>...................] - ETA: 10s - loss: 2.5405\u001b[0m\n",
      "\u001b[34m384/921 [===========>..................] - ETA: 9s - loss: 2.4621 \u001b[0m\n",
      "\u001b[34m416/921 [============>.................] - ETA: 8s - loss: 2.3677\u001b[0m\n",
      "\u001b[34m448/921 [=============>................] - ETA: 7s - loss: 2.2522\u001b[0m\n",
      "\u001b[34m480/921 [==============>...............] - ETA: 6s - loss: 2.1363\u001b[0m\n",
      "\u001b[34m512/921 [===============>..............] - ETA: 5s - loss: 2.0249\u001b[0m\n",
      "\u001b[34m544/921 [================>.............] - ETA: 5s - loss: 1.9303\u001b[0m\n",
      "\u001b[34m576/921 [=================>............] - ETA: 4s - loss: 1.8461\u001b[0m\n",
      "\u001b[34m608/921 [==================>...........] - ETA: 3s - loss: 1.7670\u001b[0m\n",
      "\u001b[34m640/921 [===================>..........] - ETA: 3s - loss: 1.7092\u001b[0m\n",
      "\u001b[34m672/921 [====================>.........] - ETA: 2s - loss: 1.6552\u001b[0m\n",
      "\u001b[34m704/921 [=====================>........] - ETA: 2s - loss: 1.6059\u001b[0m\n",
      "\u001b[34m736/921 [======================>.......] - ETA: 2s - loss: 1.5553\u001b[0m\n",
      "\u001b[34m768/921 [========================>.....] - ETA: 1s - loss: 1.5038\u001b[0m\n",
      "\u001b[34m800/921 [=========================>....] - ETA: 1s - loss: 1.4612\u001b[0m\n",
      "\u001b[34m832/921 [==========================>...] - ETA: 0s - loss: 1.4226\u001b[0m\n",
      "\u001b[34m864/921 [===========================>..] - ETA: 0s - loss: 1.3905\u001b[0m\n",
      "\u001b[34m896/921 [============================>.] - ETA: 0s - loss: 1.3620\u001b[0m\n",
      "\u001b[34m921/921 [==============================] - 10s 11ms/step - loss: 1.3337 - val_loss: 0.4080\u001b[0m\n",
      "\u001b[34mEpoch 2/3\n",
      "\n",
      " 32/921 [>.............................] - ETA: 4s - loss: 0.4073\n",
      " 64/921 [=>............................] - ETA: 4s - loss: 0.3879\n",
      " 96/921 [==>...........................] - ETA: 4s - loss: 0.4086\u001b[0m\n",
      "\u001b[34m128/921 [===>..........................] - ETA: 4s - loss: 0.4394\u001b[0m\n",
      "\u001b[34m160/921 [====>.........................] - ETA: 4s - loss: 0.4242\u001b[0m\n",
      "\u001b[34m192/921 [=====>........................] - ETA: 3s - loss: 0.4138\u001b[0m\n",
      "\u001b[34m224/921 [======>.......................] - ETA: 3s - loss: 0.4174\u001b[0m\n",
      "\u001b[34m256/921 [=======>......................] - ETA: 3s - loss: 0.4181\u001b[0m\n",
      "\u001b[34m288/921 [========>.....................] - ETA: 3s - loss: 0.4140\u001b[0m\n",
      "\u001b[34m320/921 [=========>....................] - ETA: 3s - loss: 0.4078\u001b[0m\n",
      "\u001b[34m352/921 [==========>...................] - ETA: 3s - loss: 0.4044\u001b[0m\n",
      "\u001b[34m384/921 [===========>..................] - ETA: 2s - loss: 0.4035\u001b[0m\n",
      "\u001b[34m416/921 [============>.................] - ETA: 2s - loss: 0.4011\u001b[0m\n",
      "\u001b[34m448/921 [=============>................] - ETA: 2s - loss: 0.3988\u001b[0m\n",
      "\u001b[34m480/921 [==============>...............] - ETA: 2s - loss: 0.3994\u001b[0m\n",
      "\u001b[34m512/921 [===============>..............] - ETA: 2s - loss: 0.4008\u001b[0m\n",
      "\u001b[34m544/921 [================>.............] - ETA: 2s - loss: 0.4009\u001b[0m\n",
      "\u001b[34m576/921 [=================>............] - ETA: 1s - loss: 0.4005\u001b[0m\n",
      "\u001b[34m608/921 [==================>...........] - ETA: 1s - loss: 0.3983\u001b[0m\n",
      "\u001b[34m640/921 [===================>..........] - ETA: 1s - loss: 0.3945\u001b[0m\n",
      "\u001b[34m672/921 [====================>.........] - ETA: 1s - loss: 0.3916\u001b[0m\n",
      "\u001b[34m704/921 [=====================>........] - ETA: 1s - loss: 0.3876\u001b[0m\n",
      "\u001b[34m736/921 [======================>.......] - ETA: 0s - loss: 0.3900\u001b[0m\n",
      "\u001b[34m768/921 [========================>.....] - ETA: 0s - loss: 0.3877\u001b[0m\n",
      "\u001b[34m800/921 [=========================>....] - ETA: 0s - loss: 0.3855\u001b[0m\n",
      "\u001b[34m832/921 [==========================>...] - ETA: 0s - loss: 0.3856\u001b[0m\n",
      "\u001b[34m864/921 [===========================>..] - ETA: 0s - loss: 0.3837\u001b[0m\n",
      "\u001b[34m896/921 [============================>.] - ETA: 0s - loss: 0.3810\u001b[0m\n",
      "\u001b[34m921/921 [==============================] - 5s 6ms/step - loss: 0.3792 - val_loss: 0.3227\u001b[0m\n",
      "\u001b[34mEpoch 3/3\n",
      "\n",
      " 32/921 [>.............................] - ETA: 4s - loss: 0.3287\n",
      " 64/921 [=>............................] - ETA: 4s - loss: 0.3393\u001b[0m\n",
      "\u001b[34m 96/921 [==>...........................] - ETA: 4s - loss: 0.3386\u001b[0m\n",
      "\u001b[34m128/921 [===>..........................] - ETA: 4s - loss: 0.3294\u001b[0m\n",
      "\u001b[34m160/921 [====>.........................] - ETA: 4s - loss: 0.3254\u001b[0m\n",
      "\u001b[34m192/921 [=====>........................] - ETA: 3s - loss: 0.3402\u001b[0m\n",
      "\u001b[34m224/921 [======>.......................] - ETA: 3s - loss: 0.3329\u001b[0m\n",
      "\u001b[34m256/921 [=======>......................] - ETA: 3s - loss: 0.3349\u001b[0m\n",
      "\u001b[34m288/921 [========>.....................] - ETA: 3s - loss: 0.3255\u001b[0m\n",
      "\u001b[34m320/921 [=========>....................] - ETA: 3s - loss: 0.3340\u001b[0m\n",
      "\u001b[34m352/921 [==========>...................] - ETA: 3s - loss: 0.3310\u001b[0m\n",
      "\u001b[34m384/921 [===========>..................] - ETA: 2s - loss: 0.3301\u001b[0m\n",
      "\u001b[34m416/921 [============>.................] - ETA: 2s - loss: 0.3253\u001b[0m\n",
      "\u001b[34m448/921 [=============>................] - ETA: 2s - loss: 0.3238\u001b[0m\n",
      "\u001b[34m480/921 [==============>...............] - ETA: 2s - loss: 0.3260\u001b[0m\n",
      "\u001b[34m512/921 [===============>..............] - ETA: 2s - loss: 0.3292\u001b[0m\n",
      "\u001b[34m544/921 [================>.............] - ETA: 2s - loss: 0.3302\u001b[0m\n",
      "\u001b[34m576/921 [=================>............] - ETA: 1s - loss: 0.3355\u001b[0m\n",
      "\u001b[34m608/921 [==================>...........] - ETA: 1s - loss: 0.3362\u001b[0m\n",
      "\u001b[34m640/921 [===================>..........] - ETA: 1s - loss: 0.3373\u001b[0m\n",
      "\u001b[34m672/921 [====================>.........] - ETA: 1s - loss: 0.3360\u001b[0m\n",
      "\u001b[34m704/921 [=====================>........] - ETA: 1s - loss: 0.3352\u001b[0m\n",
      "\u001b[34m736/921 [======================>.......] - ETA: 0s - loss: 0.3380\u001b[0m\n",
      "\u001b[34m768/921 [========================>.....] - ETA: 0s - loss: 0.3367\u001b[0m\n",
      "\u001b[34m800/921 [=========================>....] - ETA: 0s - loss: 0.3358\u001b[0m\n",
      "\u001b[34m832/921 [==========================>...] - ETA: 0s - loss: 0.3344\u001b[0m\n",
      "\u001b[34m864/921 [===========================>..] - ETA: 0s - loss: 0.3356\u001b[0m\n",
      "\u001b[34m896/921 [============================>.] - ETA: 0s - loss: 0.3364\u001b[0m\n",
      "\u001b[34m921/921 [==============================] - 5s 6ms/step - loss: 0.3362 - val_loss: 0.3000\u001b[0m\n",
      "\u001b[34mWARNING: Logging before flag parsing goes to stderr.\u001b[0m\n",
      "\u001b[34mW0522 09:16:00.800822 139884849997568 training.py:181] No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\u001b[0m\n",
      "\u001b[34mFor details of how to construct your training script see:\u001b[0m\n",
      "\u001b[34mhttps://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/tensorflow#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\n",
      "2020-05-22 09:16:12 Uploading - Uploading generated training model\n",
      "2020-05-22 09:16:12 Completed - Training job completed\n",
      "Training seconds: 128\n",
      "Billable seconds: 128\n"
     ]
    }
   ],
   "source": [
    "tf_estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "def all_done():\n",
    "    display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_predictor = tf_estimator.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.m4.xlarge',\n",
    "                                   endpoint_type='tensorflow-serving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "def all_done():\n",
    "    display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf_predictor.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_f1 = f1_score(predictions, y_te)\n",
    "print(f\"Test F1-Score: {test_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
