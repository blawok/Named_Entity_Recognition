{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for RoBERTa model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:  \n",
    "    - https://huggingface.co/transformers/_modules/transformers/tokenization_roberta.html#RobertaTokenizer  \n",
    "    - https://www.kaggle.com/debanga/huggingface-tokenizers-cheat-sheet  \n",
    "    - https://github.com/billpku/NLP_In_Action  \n",
    "    - https://androidkt.com/name-entity-recognition-with-bert-in-tensorflow/  \n",
    "    - https://github.com/smart-patrol/sagemaker-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (2.11.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers) (4.46.1)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers) (2020.5.14)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers) (3.0.4)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers) (1.16.4)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers) (2.20.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sacremoses->transformers) (6.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from packaging->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->transformers) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->transformers) (1.23)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither PyTorch nor TensorFlow >= 2.0 have been found.Models won't be available and only tokenizers, configurationand file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import os\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer\n",
    "from helpers import SentenceGetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: sagemaker-eu-west-1-087816224558\n",
      "Role: arn:aws:iam::087816224558:role/service-role/AmazonSageMaker-ExecutionRole-20200424T125478\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f'Bucket: {bucket}')\n",
    "print(f'Role: {role}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../data/interim/ner_dataset.csv\", encoding=\"latin1\").fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048575, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform data into sentences and labels using SentenceGetter from helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence with tags:\n",
      "Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n"
     ]
    }
   ],
   "source": [
    "getter = SentenceGetter(data)\n",
    "\n",
    "sentences = [[s[0] for s in sent] for sent in getter.sentences]\n",
    "labels = [[s[2] for s in sent] for sent in getter.sentences]\n",
    "\n",
    "print(\"Example sentence with tags:\")\n",
    "print(\" \".join(sentences[0]))\n",
    "print(\" \".join(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = list(set(data[\"Tag\"].values))\n",
    "\n",
    "# add X label for word piece support, as well as [CLS] and [SEP] - as it is required by RoBERTa\n",
    "tags_vals.append('X')\n",
    "tags_vals.append('[CLS]')\n",
    "tags_vals.append('[SEP]')\n",
    "\n",
    "# ensure that we've got unique tags\n",
    "tags_vals = set(tags_vals)\n",
    "\n",
    "# hardcode tag2idx dictionary for easier reproducibility\n",
    "tag2idx={\n",
    "     'B-art': 14,\n",
    "     'B-eve': 16,\n",
    "     'B-geo': 0,\n",
    "     'B-gpe': 13,\n",
    "     'B-nat': 12,\n",
    "     'B-org': 10,\n",
    "     'B-per': 4,\n",
    "     'B-tim': 2,\n",
    "     'I-art': 5,\n",
    "     'I-eve': 7,\n",
    "     'I-geo': 15,\n",
    "     'I-gpe': 8,\n",
    "     'I-nat': 11,\n",
    "     'I-org': 3,\n",
    "     'I-per': 6,\n",
    "     'I-tim': 1,\n",
    "     'X':17,\n",
    "     'O': 9,\n",
    "     '[CLS]':18,\n",
    "     '[SEP]':19\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set maximum sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN  = 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize for RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer for RoBERTa from Hugging Face library\n",
    "tokenizer=RobertaTokenizer.from_pretrained('roberta-base',\n",
    "                                           do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RoBERTa to work properly, we need to add CLS and SEP tokens at the beggining and at the end respectively.  \n",
    "Each word has to be also tokenized using RobertaTokenizer (splitted into word pieces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized example:\n",
      "[CLS] The United N ations is ... [SEP]\n",
      "[CLS] O B-org I-org X O ... [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = []\n",
    "word_piece_labels = []\n",
    "\n",
    "for word_list,label in (zip(sentences,labels)):\n",
    "    temp_label = []\n",
    "    temp_token = []\n",
    "    \n",
    "    # add [CLS] token at the front \n",
    "    temp_label.append('[CLS]')\n",
    "    temp_token.append('[CLS]')\n",
    "    \n",
    "    for word,lab in zip(word_list,label):\n",
    "        token_list = tokenizer.tokenize(word)\n",
    "        for m,token in enumerate(token_list):\n",
    "            temp_token.append(token)\n",
    "            if m==0:\n",
    "                temp_label.append(lab)\n",
    "            else:\n",
    "                temp_label.append('X')  \n",
    "                \n",
    "    # add [SEP] token at the end\n",
    "    temp_label.append('[SEP]')\n",
    "    temp_token.append('[SEP]')\n",
    "    \n",
    "    tokenized_texts.append(temp_token)\n",
    "    word_piece_labels.append(temp_label)\n",
    "    \n",
    "print('Tokenized example:')\n",
    "print(\" \".join(temp_token[:6]), \"...\", temp_token[-1])\n",
    "print(\" \".join(temp_label[:6]), \"...\", temp_token[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, convert tokenized sequences into ids sequences from RobertaTokenized and add padding (or cut sequence) to MAX_LEN. Add tags for padded sequences - \"O\" tag (no meaning), here it is number 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokenized text in form of RoBERTa ids:\n",
      "[    3 33383  1116 34084  6031  3629 11990  3916  3804 11672 23122   560\n",
      "  4892 21959   627  5557   179 37590   463 15509   627  5632 24686   337\n",
      "  1116 24270 19937  5090  7761  6025 12659     4     3     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0]\n",
      "\n",
      "Tags for the example text:\n",
      "[18  9  9  9 17 17  9  9 17  9  0  9  9 17  9  9  9  0  9  9  9  9 17 17\n",
      "  9 13  9 17  9  9  9  9 19  9  9  9  9  9  9  9  9  9  9  9  9]\n"
     ]
    }
   ],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in word_piece_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "print('Example tokenized text in form of RoBERTa ids:')\n",
    "print(input_ids[0])\n",
    "print(\"\")\n",
    "print('Tags for the example text:')\n",
    "print(tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model to recognize padded values we need to add masks (1 for real tokens and 0 for padded values). RoBERTa is also suited for multi-sequence input thanks to segments, but in our case inputs are single sentences, so we need to simply add the same segment id to each sequence (zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "attention_masks = [[int(i>0) for i in ii] for ii in input_ids]\n",
    "segment_ids = [[0] * len(input_id) for input_id in input_ids]\n",
    "\n",
    "print(attention_masks[0])\n",
    "print(segment_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split data into the same splits that we did in BiLSTM preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags, tr_masks, val_masks, tr_segs, val_segs = train_test_split(input_ids,\n",
    "                                                                                                    tags,\n",
    "                                                                                                    attention_masks,\n",
    "                                                                                                    segment_ids, \n",
    "                                                                                                    random_state=666,\n",
    "                                                                                                    test_size=0.1)\n",
    "# check shapes\n",
    "assert len(tr_inputs) == len(tr_segs) == len(tr_masks)\n",
    "assert len(val_inputs) == len(val_segs) == len(val_masks)\n",
    "assert tr_inputs[0].shape[0] == 45\n",
    "assert val_inputs[0].shape[0] == 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(tr_tags),\n",
    "           pd.DataFrame(tr_inputs),\n",
    "           pd.DataFrame(tr_masks),\n",
    "           pd.DataFrame(tr_segs)], axis=1).to_csv(os.path.join('../../data/processed_roberta/', 'train_roberta.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([pd.DataFrame(val_tags),\n",
    "           pd.DataFrame(val_inputs),\n",
    "           pd.DataFrame(val_masks),\n",
    "           pd.DataFrame(val_segs)], axis=1).to_csv(os.path.join('../../data/processed_roberta/', 'test_roberta.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current data directory\n",
    "DATA_DIR = '../../data/processed_roberta'\n",
    "\n",
    "# data directory in S3\n",
    "PREFIX = 'named_entity_recognition/roberta_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=DATA_DIR, bucket=bucket, key_prefix=PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to save directories to those files on S3 for training purposes\n",
    "data_directories = {'train_data_directory': os.path.join(input_data, 'test_roberta.csv'),\n",
    "                    'test_data_directory': os.path.join(input_data, 'test_roberta.csv')\n",
    "                   }\n",
    "\n",
    "data_directories_file = open(\"../../src/utils/objects/data_directories_roberta.json\", \"w\")\n",
    "json.dump(data_directories, data_directories_file)\n",
    "data_directories_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty Bucket (optional)\n",
    "\n",
    "# import boto3\n",
    "# bucket_to_delete = boto3.resource('s3').Bucket(bucket)\n",
    "# bucket_to_delete.objects.all().delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
